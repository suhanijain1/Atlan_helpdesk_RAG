{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9feb7d5",
   "metadata": {},
   "source": [
    "# Comparing HDBSCAN Clustering vs Gemini Classifications\n",
    "\n",
    "Let's compare the unsupervised clustering results with Gemini's supervised topic classification to see:\n",
    "1. How well they align\n",
    "2. Which approach gives better insights\n",
    "3. Whether clustering can validate/improve Gemini's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "879a49e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARING TWO TRIAGE APPROACHES ===\n",
      "\n",
      "🔬 APPROACH 1: Notebook Multi-Label Evaluator\n",
      "✅ Tickets processed: 30\n",
      "✅ Status breakdown: {'confident': 10, 'moderate': 13, 'disagreement': 6, 'isolated': 1}\n",
      "✅ Actions needed: {'keep': 24, 'review': 6}\n",
      "✅ High confidence rate: 33.3%\n",
      "✅ Med+High confidence rate: 80.0%\n",
      "\n",
      "🔬 APPROACH 2: triage_evaluation_full.py Framework\n",
      "✅ Tickets processed: 30\n",
      "✅ Topics identified: 10\n",
      "✅ Average topic coherence: 0.336\n",
      "✅ High-coherence topics (>0.4): 2/10\n",
      "✅ Confidence distribution:\n",
      "   MED_CONF: 20 (66.7%)\n",
      "   LOW_CONF: 5 (16.7%)\n",
      "   HIGH_CONF: 5 (16.7%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "print(\"=== COMPARING TWO TRIAGE APPROACHES ===\")\n",
    "\n",
    "# Load APPROACH 1: Notebook-based evaluation (triage.ipynb approach)\n",
    "print(\"\\n🔬 APPROACH 1: Notebook Multi-Label Evaluator\")\n",
    "try:\n",
    "    # This uses the validation results from the notebook evaluator\n",
    "    notebook_validation = validation_results  # From previous cells\n",
    "    notebook_status = Counter([r['status'] for r in notebook_validation])\n",
    "    notebook_actions = Counter([r['action'] for r in notebook_validation])\n",
    "    \n",
    "    print(f\"✅ Tickets processed: {len(notebook_validation)}\")\n",
    "    print(f\"✅ Status breakdown: {dict(notebook_status)}\")\n",
    "    print(f\"✅ Actions needed: {dict(notebook_actions)}\")\n",
    "    \n",
    "    # Calculate confidence rates\n",
    "    high_conf_nb = len([r for r in notebook_validation if r['confidence'] == 'high'])\n",
    "    med_conf_nb = len([r for r in notebook_validation if r['confidence'] == 'medium'])\n",
    "    \n",
    "    print(f\"✅ High confidence rate: {high_conf_nb/len(notebook_validation):.1%}\")\n",
    "    print(f\"✅ Med+High confidence rate: {(high_conf_nb + med_conf_nb)/len(notebook_validation):.1%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Notebook approach data not available: {e}\")\n",
    "\n",
    "# Load APPROACH 2: triage_evaluation_full.py results\n",
    "print(\"\\n🔬 APPROACH 2: triage_evaluation_full.py Framework\")\n",
    "try:\n",
    "    # Load results from validation run\n",
    "    framework_results = pd.read_csv('./validation_results/tickets_with_metrics.csv')\n",
    "    framework_topics = pd.read_csv('./validation_results/topic_summary.csv')\n",
    "    \n",
    "    print(f\"✅ Tickets processed: {len(framework_results)}\")\n",
    "    print(f\"✅ Topics identified: {len(framework_topics)}\")\n",
    "    \n",
    "    # Calculate coherence stats\n",
    "    avg_coherence = framework_topics['coherence'].mean()\n",
    "    high_coh_topics = len(framework_topics[framework_topics['coherence'] > 0.4])\n",
    "    \n",
    "    print(f\"✅ Average topic coherence: {avg_coherence:.3f}\")\n",
    "    print(f\"✅ High-coherence topics (>0.4): {high_coh_topics}/{len(framework_topics)}\")\n",
    "    \n",
    "    # Calculate confidence distribution\n",
    "    if 'conf_label' in framework_results.columns:\n",
    "        conf_dist = framework_results['conf_label'].value_counts()\n",
    "        total = len(framework_results)\n",
    "        print(f\"✅ Confidence distribution:\")\n",
    "        for conf, count in conf_dist.items():\n",
    "            print(f\"   {conf}: {count} ({count/total:.1%})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Framework approach data not available: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee3521f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DETAILED COMPARISON\n",
      "============================================================\n",
      "🎯 CONFIDENCE COMPARISON:\n",
      "   Notebook Approach:\n",
      "     • High confidence: 33.3%\n",
      "     • Med+High confidence: 80.0%\n",
      "     • Reviews needed: 6/30 (20.0%)\n",
      "   Framework Approach:\n",
      "     • High confidence: 16.7%\n",
      "     • Med+High confidence: 83.4%\n",
      "     • Reviews needed: 5/30 (16.7%)\n",
      "\n",
      "🏆 WINNER ANALYSIS:\n",
      "   ✅ High Confidence: NOTEBOOK wins (33.3% vs 16.7%)\n",
      "   ✅ Overall Confidence: FRAMEWORK wins (83.4% vs 80.0%)\n",
      "   ✅ Fewer Reviews Needed: FRAMEWORK wins (5 vs 6)\n",
      "\n",
      "📈 ADDITIONAL FRAMEWORK METRICS:\n",
      "   • Topic coherence quality: 0.336 (>0.3 is good)\n",
      "   • High-quality topics: 2/10 topics\n",
      "\n",
      "🔍 RECOMMENDATION:\n",
      "   🎯 USE FRAMEWORK APPROACH\n",
      "   ✅ More comprehensive evaluation\n",
      "   ✅ Better topic quality metrics\n",
      "   ✅ Production-ready pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE COMPARISON ANALYSIS\n",
    "print(\"📊 DETAILED COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Approach 1: Notebook Multi-Label Evaluator\n",
    "nb_high_conf = 33.3\n",
    "nb_med_high_conf = 80.0\n",
    "nb_reviews_needed = 6\n",
    "nb_total = 30\n",
    "\n",
    "# Approach 2: Framework \n",
    "fw_high_conf = 16.7\n",
    "fw_med_high_conf = 66.7 + 16.7  # MED + HIGH\n",
    "fw_reviews_needed = 5  # LOW_CONF\n",
    "fw_total = 30\n",
    "fw_avg_coherence = 0.336\n",
    "fw_high_coh_topics = 2\n",
    "\n",
    "print(\"🎯 CONFIDENCE COMPARISON:\")\n",
    "print(f\"   Notebook Approach:\")\n",
    "print(f\"     • High confidence: {nb_high_conf:.1f}%\")\n",
    "print(f\"     • Med+High confidence: {nb_med_high_conf:.1f}%\")\n",
    "print(f\"     • Reviews needed: {nb_reviews_needed}/{nb_total} ({nb_reviews_needed/nb_total:.1%})\")\n",
    "\n",
    "print(f\"   Framework Approach:\")\n",
    "print(f\"     • High confidence: {fw_high_conf:.1f}%\")\n",
    "print(f\"     • Med+High confidence: {fw_med_high_conf:.1f}%\")\n",
    "print(f\"     • Reviews needed: {fw_reviews_needed}/{fw_total} ({fw_reviews_needed/fw_total:.1%})\")\n",
    "\n",
    "print(f\"\\n🏆 WINNER ANALYSIS:\")\n",
    "if nb_high_conf > fw_high_conf:\n",
    "    print(f\"   ✅ High Confidence: NOTEBOOK wins ({nb_high_conf:.1f}% vs {fw_high_conf:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ✅ High Confidence: FRAMEWORK wins ({fw_high_conf:.1f}% vs {nb_high_conf:.1f}%)\")\n",
    "\n",
    "if nb_med_high_conf > fw_med_high_conf:\n",
    "    print(f\"   ✅ Overall Confidence: NOTEBOOK wins ({nb_med_high_conf:.1f}% vs {fw_med_high_conf:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ✅ Overall Confidence: FRAMEWORK wins ({fw_med_high_conf:.1f}% vs {nb_med_high_conf:.1f}%)\")\n",
    "\n",
    "if nb_reviews_needed < fw_reviews_needed:\n",
    "    print(f\"   ✅ Fewer Reviews Needed: NOTEBOOK wins ({nb_reviews_needed} vs {fw_reviews_needed})\")\n",
    "else:\n",
    "    print(f\"   ✅ Fewer Reviews Needed: FRAMEWORK wins ({fw_reviews_needed} vs {nb_reviews_needed})\")\n",
    "\n",
    "print(f\"\\n📈 ADDITIONAL FRAMEWORK METRICS:\")\n",
    "print(f\"   • Topic coherence quality: {fw_avg_coherence:.3f} (>0.3 is good)\")\n",
    "print(f\"   • High-quality topics: {fw_high_coh_topics}/10 topics\")\n",
    "\n",
    "print(f\"\\n🔍 RECOMMENDATION:\")\n",
    "if nb_med_high_conf > fw_med_high_conf and nb_reviews_needed < fw_reviews_needed:\n",
    "    print(f\"   🎯 USE NOTEBOOK APPROACH\")\n",
    "    print(f\"   ✅ Better confidence rates\")\n",
    "    print(f\"   ✅ Fewer manual reviews needed\")\n",
    "    print(f\"   ✅ More suitable for production\")\n",
    "else:\n",
    "    print(f\"   🎯 USE FRAMEWORK APPROACH\") \n",
    "    print(f\"   ✅ More comprehensive evaluation\")\n",
    "    print(f\"   ✅ Better topic quality metrics\")\n",
    "    print(f\"   ✅ Production-ready pipeline\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61e7c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 CHECKING ALIGNMENT WITH GEMINI'S ORIGINAL CONFIDENCE\n",
      "============================================================\n",
      "📊 Gemini's Original Confidence Distribution:\n",
      "   Mean: 0.888\n",
      "   Std:  0.036\n",
      "   Min:  0.800\n",
      "   Max:  0.950\n",
      "\n",
      "🔢 Gemini's Confidence Breakdown:\n",
      "   High confidence (≥0.9): 26/30 (86.7%)\n",
      "   Med+ confidence (≥0.8): 30/30 (100.0%)\n",
      "\n",
      "📈 ALIGNMENT COMPARISON:\n",
      "   Gemini High Conf:    86.7%\n",
      "   Notebook High Conf:  33.3%\n",
      "   Framework High Conf: 16.7%\n",
      "\n",
      "🎯 ALIGNMENT SCORES (lower = better):\n",
      "   Notebook alignment error:  0.534\n",
      "   Framework alignment error: 0.700\n",
      "   🏆 NOTEBOOK aligns better with Gemini confidence!\n",
      "\n",
      "============================================================\n",
      "🎯 FINAL RECOMMENDATION BASED ON ALL METRICS:\n",
      "✅ Notebook: Better Gemini alignment (+2)\n",
      "✅ Framework: Higher overall confidence (+1)\n",
      "✅ Framework: Fewer reviews needed (+1)\n",
      "✅ Framework: Production-ready pipeline (+2)\n",
      "✅ Framework: Topic coherence metrics (+1)\n",
      "\n",
      "📊 FINAL SCORES:\n",
      "   Notebook Score:  2\n",
      "   Framework Score: 5\n",
      "\n",
      "🏆 WINNER: FRAMEWORK APPROACH\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# GEMINI CONFIDENCE ALIGNMENT CHECK\n",
    "print(\"🎯 CHECKING ALIGNMENT WITH GEMINI'S ORIGINAL CONFIDENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Gemini's original confidence scores\n",
    "gemini_df = pd.read_csv('Ticket_Classification_Results_CLEAN.csv')\n",
    "gemini_confidence = gemini_df['confidence'].describe()\n",
    "\n",
    "print(\"📊 Gemini's Original Confidence Distribution:\")\n",
    "print(f\"   Mean: {gemini_confidence['mean']:.3f}\")\n",
    "print(f\"   Std:  {gemini_confidence['std']:.3f}\")\n",
    "print(f\"   Min:  {gemini_confidence['min']:.3f}\")\n",
    "print(f\"   Max:  {gemini_confidence['max']:.3f}\")\n",
    "\n",
    "# Check high confidence tickets in Gemini\n",
    "high_conf_gemini = len(gemini_df[gemini_df['confidence'] >= 0.9])\n",
    "med_conf_gemini = len(gemini_df[gemini_df['confidence'] >= 0.8])\n",
    "\n",
    "print(f\"\\n🔢 Gemini's Confidence Breakdown:\")\n",
    "print(f\"   High confidence (≥0.9): {high_conf_gemini}/30 ({high_conf_gemini/30:.1%})\")\n",
    "print(f\"   Med+ confidence (≥0.8): {med_conf_gemini}/30 ({med_conf_gemini/30:.1%})\")\n",
    "\n",
    "print(f\"\\n📈 ALIGNMENT COMPARISON:\")\n",
    "print(f\"   Gemini High Conf:    {high_conf_gemini/30:.1%}\")\n",
    "print(f\"   Notebook High Conf:  {nb_high_conf:.1f}%\")\n",
    "print(f\"   Framework High Conf: {fw_high_conf:.1f}%\")\n",
    "\n",
    "notebook_alignment = abs((nb_high_conf/100) - (high_conf_gemini/30))\n",
    "framework_alignment = abs((fw_high_conf/100) - (high_conf_gemini/30))\n",
    "\n",
    "print(f\"\\n🎯 ALIGNMENT SCORES (lower = better):\")\n",
    "print(f\"   Notebook alignment error:  {notebook_alignment:.3f}\")\n",
    "print(f\"   Framework alignment error: {framework_alignment:.3f}\")\n",
    "\n",
    "if notebook_alignment < framework_alignment:\n",
    "    print(f\"   🏆 NOTEBOOK aligns better with Gemini confidence!\")\n",
    "else:\n",
    "    print(f\"   🏆 FRAMEWORK aligns better with Gemini confidence!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 FINAL RECOMMENDATION BASED ON ALL METRICS:\")\n",
    "\n",
    "# Score each approach\n",
    "nb_score = 0\n",
    "fw_score = 0\n",
    "\n",
    "# High confidence rate (closer to Gemini)\n",
    "if notebook_alignment < framework_alignment:\n",
    "    nb_score += 2\n",
    "    print(\"✅ Notebook: Better Gemini alignment (+2)\")\n",
    "else:\n",
    "    fw_score += 2\n",
    "    print(\"✅ Framework: Better Gemini alignment (+2)\")\n",
    "\n",
    "# Overall confidence\n",
    "if nb_med_high_conf > fw_med_high_conf:\n",
    "    nb_score += 1\n",
    "    print(\"✅ Notebook: Higher overall confidence (+1)\")\n",
    "else:\n",
    "    fw_score += 1\n",
    "    print(\"✅ Framework: Higher overall confidence (+1)\")\n",
    "\n",
    "# Fewer reviews needed\n",
    "if nb_reviews_needed < fw_reviews_needed:\n",
    "    nb_score += 1\n",
    "    print(\"✅ Notebook: Fewer reviews needed (+1)\")\n",
    "else:\n",
    "    fw_score += 1\n",
    "    print(\"✅ Framework: Fewer reviews needed (+1)\")\n",
    "\n",
    "# Production readiness\n",
    "fw_score += 2\n",
    "print(\"✅ Framework: Production-ready pipeline (+2)\")\n",
    "\n",
    "# Topic quality metrics\n",
    "fw_score += 1\n",
    "print(\"✅ Framework: Topic coherence metrics (+1)\")\n",
    "\n",
    "print(f\"\\n📊 FINAL SCORES:\")\n",
    "print(f\"   Notebook Score:  {nb_score}\")\n",
    "print(f\"   Framework Score: {fw_score}\")\n",
    "\n",
    "if nb_score > fw_score:\n",
    "    winner = \"NOTEBOOK\"\n",
    "else:\n",
    "    winner = \"FRAMEWORK\"\n",
    "\n",
    "print(f\"\\n🏆 WINNER: {winner} APPROACH\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27f58f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b0cf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-lite',\n",
    "    contents='Capital of france is?'\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bb98b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 tickets from sample_tickets.json\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "# Define topic tags as Enum\n",
    "class TopicTag(str, Enum):\n",
    "    HOW_TO = \"How-to\"\n",
    "    PRODUCT = \"Product\"\n",
    "    CONNECTOR = \"Connector\"\n",
    "    LINEAGE = \"Lineage\"\n",
    "    API_SDK = \"API/SDK\"\n",
    "    SSO = \"SSO\"\n",
    "    GLOSSARY = \"Glossary\"\n",
    "    BEST_PRACTICES = \"Best practices\"\n",
    "    SENSITIVE_DATA = \"Sensitive data\"\n",
    "    OTHER = \"Other\"\n",
    "\n",
    "class TicketFeatures(BaseModel):\n",
    "    topics: list[TopicTag] = Field(min_length=1, description=\"List of relevant topics for the ticket\")\n",
    "    sentiment_score: float = Field(ge=-1.0, le=1.0)\n",
    "    sentiment_label: str\n",
    "    urgency_score: float = Field(ge=0.0, le=1.0)\n",
    "    priority: str\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    key_entities: list[str]\n",
    "    reasoning: str\n",
    "\n",
    "# Load sample tickets\n",
    "with open('sample_tickets.json', 'r') as f:\n",
    "    tickets = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(tickets)} tickets from sample_tickets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d5eb17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tickets 1 to 15 of 30\n",
      "Batch complete. Waiting 60 seconds before next batch...\n",
      "Batch complete. Waiting 60 seconds before next batch...\n",
      "Processing tickets 16 to 30 of 30\n",
      "Processing tickets 16 to 30 of 30\n",
      "Successfully processed 30 tickets\n",
      "Saved cleaned results to Ticket_Classification_Results_CLEAN.csv\n",
      "Successfully processed 30 tickets\n",
      "Saved cleaned results to Ticket_Classification_Results_CLEAN.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>topics</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>urgency_score</th>\n",
       "      <th>priority</th>\n",
       "      <th>confidence</th>\n",
       "      <th>key_entities</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>classification_success</th>\n",
       "      <th>topics_str</th>\n",
       "      <th>key_entities_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-245</td>\n",
       "      <td>Connecting Snowflake to Atlan - required permi...</td>\n",
       "      <td>Hi team, we're trying to set up our primary Sn...</td>\n",
       "      <td>[TopicTag.CONNECTOR, TopicTag.HOW_TO]</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.90</td>\n",
       "      <td>P0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>[Snowflake, Atlan, BI team]</td>\n",
       "      <td>The user is experiencing a connection failure ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Connector, How-to</td>\n",
       "      <td>Snowflake, Atlan, BI team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-246</td>\n",
       "      <td>Which connectors automatically capture lineage?</td>\n",
       "      <td>Hello, I'm new to Atlan and trying to understa...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.CONNECTOR, TopicTag...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.70</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>[Fivetran, dbt, Tableau]</td>\n",
       "      <td>The user is asking for a 'how-to' on lineage c...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Connector, Lineage</td>\n",
       "      <td>Fivetran, dbt, Tableau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-247</td>\n",
       "      <td>Deployment of Atlan agent for private data lake</td>\n",
       "      <td>Our primary data lake is hosted on-premise wit...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.CONNECTOR]</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.90</td>\n",
       "      <td>P0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan agent, VPC]</td>\n",
       "      <td>The user is asking for help with setting up th...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Connector</td>\n",
       "      <td>Atlan agent, VPC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-248</td>\n",
       "      <td>How to surface sample rows and schema changes?</td>\n",
       "      <td>Hi, we've successfully connected our Redshift ...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.CONNECTOR]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Redshift]</td>\n",
       "      <td>The user is asking how to perform a task ('how...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Connector</td>\n",
       "      <td>Redshift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TICKET-249</td>\n",
       "      <td>Exporting lineage view for a specific table</td>\n",
       "      <td>For our quarterly audit, I need to provide a c...</td>\n",
       "      <td>[TopicTag.LINEAGE, TopicTag.HOW_TO]</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.80</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[fact_orders table, lineage diagram]</td>\n",
       "      <td>The user needs to export lineage information f...</td>\n",
       "      <td>True</td>\n",
       "      <td>Lineage, How-to</td>\n",
       "      <td>fact_orders table, lineage diagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TICKET-250</td>\n",
       "      <td>Importing lineage from Airflow jobs</td>\n",
       "      <td>We run hundreds of ETL jobs in Airflow, and we...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.CONNECTOR, TopicTag...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>[Airflow, ETL, DAGs, datasets]</td>\n",
       "      <td>The user is asking a 'how-to' question about i...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Connector, Lineage</td>\n",
       "      <td>Airflow, ETL, DAGs, datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TICKET-251</td>\n",
       "      <td>Using the Visual Query Builder</td>\n",
       "      <td>I'm a business analyst and not very comfortabl...</td>\n",
       "      <td>[TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Visual Query Builder, SQL]</td>\n",
       "      <td>The user is asking for instructions on how to ...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to</td>\n",
       "      <td>Visual Query Builder, SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TICKET-252</td>\n",
       "      <td>Programmatic extraction of lineage</td>\n",
       "      <td>Our internal data science team wants to build ...</td>\n",
       "      <td>[TopicTag.LINEAGE, TopicTag.API_SDK]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[API, lineage]</td>\n",
       "      <td>The user is asking how to programmatically ext...</td>\n",
       "      <td>True</td>\n",
       "      <td>Lineage, API/SDK</td>\n",
       "      <td>API, lineage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TICKET-253</td>\n",
       "      <td>Upstream lineage to Snowflake view not working</td>\n",
       "      <td>This is infuriating. We have a critical Snowfl...</td>\n",
       "      <td>[TopicTag.PRODUCT, TopicTag.CONNECTOR, TopicTa...</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.95</td>\n",
       "      <td>P0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Snowflake, lineage, crawler]</td>\n",
       "      <td>The user expresses strong dissatisfaction ('in...</td>\n",
       "      <td>True</td>\n",
       "      <td>Product, Connector, Lineage</td>\n",
       "      <td>Snowflake, lineage, crawler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TICKET-254</td>\n",
       "      <td>How to create a business glossary and link ter...</td>\n",
       "      <td>We are migrating our existing business glossar...</td>\n",
       "      <td>[TopicTag.GLOSSARY, TopicTag.API_SDK, TopicTag...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.80</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, CSV, API, Governance]</td>\n",
       "      <td>The user is asking how to bulk import glossary...</td>\n",
       "      <td>True</td>\n",
       "      <td>Glossary, API/SDK, How-to</td>\n",
       "      <td>Atlan, CSV, API, Governance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TICKET-255</td>\n",
       "      <td>Creating a custom role for data stewards</td>\n",
       "      <td>I'm trying to set up a custom role for our dat...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.GLOSSARY]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[custom role, data stewards, permissions, glos...</td>\n",
       "      <td>The user is asking a 'how-to' question about s...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Glossary</td>\n",
       "      <td>custom role, data stewards, permissions, gloss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TICKET-256</td>\n",
       "      <td>Mapping Active Directory groups to Atlan teams</td>\n",
       "      <td>Our company policy requires us to manage all u...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.SSO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>[Active Directory, AD groups, Atlan teams, per...</td>\n",
       "      <td>The user is asking how to configure Atlan to m...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, SSO</td>\n",
       "      <td>Active Directory, AD groups, Atlan teams, perm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TICKET-257</td>\n",
       "      <td>RBAC for assets vs. glossaries</td>\n",
       "      <td>I need clarification on how Atlan's role-based...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.GLOSSARY, TopicTag....</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, role-based access control, Snowflake, ...</td>\n",
       "      <td>The user is asking a 'how-to' question about A...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Glossary, Sensitive data</td>\n",
       "      <td>Atlan, role-based access control, Snowflake, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TICKET-258</td>\n",
       "      <td>Process for onboarding asset owners</td>\n",
       "      <td>We've started identifying owners for our key d...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.GLOSSARY]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[owners, data assets, Atlan, notifications]</td>\n",
       "      <td>The user is asking how to perform a task in At...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Glossary</td>\n",
       "      <td>owners, data assets, Atlan, notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TICKET-259</td>\n",
       "      <td>How does Atlan surface sensitive fields like PII?</td>\n",
       "      <td>Our security team is evaluating Atlan and thei...</td>\n",
       "      <td>[TopicTag.SENSITIVE_DATA, TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[PII, Atlan]</td>\n",
       "      <td>The user is asking 'how-to' questions about id...</td>\n",
       "      <td>True</td>\n",
       "      <td>Sensitive data, How-to</td>\n",
       "      <td>PII, Atlan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TICKET-260</td>\n",
       "      <td>Authentication methods for APIs and SDKs</td>\n",
       "      <td>We are planning to build several automations u...</td>\n",
       "      <td>[TopicTag.API_SDK, TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan API, Python SDK, OAuth, API keys]</td>\n",
       "      <td>The user is asking \"how-to\" questions about us...</td>\n",
       "      <td>True</td>\n",
       "      <td>API/SDK, How-to</td>\n",
       "      <td>Atlan API, Python SDK, OAuth, API keys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TICKET-261</td>\n",
       "      <td>Enabling and testing SAML SSO</td>\n",
       "      <td>We are ready to enable SAML SSO with our Okta ...</td>\n",
       "      <td>[TopicTag.SSO, TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.60</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[SAML, SSO, Okta]</td>\n",
       "      <td>The user is asking how to test an SSO configur...</td>\n",
       "      <td>True</td>\n",
       "      <td>SSO, How-to</td>\n",
       "      <td>SAML, SSO, Okta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TICKET-262</td>\n",
       "      <td>SSO login not assigning user to correct group</td>\n",
       "      <td>I've just had a new user, 'test.user@company.c...</td>\n",
       "      <td>[TopicTag.SSO, TopicTag.PRODUCT]</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.70</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[SSO, SAML, group assignment]</td>\n",
       "      <td>The user is experiencing an issue with SSO gro...</td>\n",
       "      <td>True</td>\n",
       "      <td>SSO, Product</td>\n",
       "      <td>SSO, SAML, group assignment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TICKET-263</td>\n",
       "      <td>Integration with existing DLP or secrets manager</td>\n",
       "      <td>Does Atlan have the capability to integrate wi...</td>\n",
       "      <td>[TopicTag.CONNECTOR, TopicTag.SENSITIVE_DATA]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, DLP, HashiCorp Vault, secrets manager]</td>\n",
       "      <td>The user is asking about integration capabilit...</td>\n",
       "      <td>True</td>\n",
       "      <td>Connector, Sensitive data</td>\n",
       "      <td>Atlan, DLP, HashiCorp Vault, secrets manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TICKET-264</td>\n",
       "      <td>Accessing audit logs for compliance reviews</td>\n",
       "      <td>Our compliance team needs to perform a quarter...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.API_SDK, TopicTag.S...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[audit logs, API]</td>\n",
       "      <td>The user is asking how to access and export au...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, API/SDK, Sensitive data</td>\n",
       "      <td>audit logs, API</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TICKET-265</td>\n",
       "      <td>How to programmatically create an asset using ...</td>\n",
       "      <td>I'm trying to create a new custom asset (a 'Re...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.API_SDK]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[REST API, custom asset, Report]</td>\n",
       "      <td>The user is asking for guidance on how to use ...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, API/SDK</td>\n",
       "      <td>REST API, custom asset, Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TICKET-266</td>\n",
       "      <td>SDK availability and Python example</td>\n",
       "      <td>I'm a data engineer and prefer using SDKs over...</td>\n",
       "      <td>[TopicTag.API_SDK, TopicTag.HOW_TO, TopicTag.G...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[SDK, API, Python, PyPI, glossary term]</td>\n",
       "      <td>The user is asking a 'how-to' question about S...</td>\n",
       "      <td>True</td>\n",
       "      <td>API/SDK, How-to, Glossary</td>\n",
       "      <td>SDK, API, Python, PyPI, glossary term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TICKET-267</td>\n",
       "      <td>How do webhooks work in Atlan?</td>\n",
       "      <td>I'm exploring using webhooks to send real-time...</td>\n",
       "      <td>[TopicTag.API_SDK, TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[webhooks, Atlan, Slack, API]</td>\n",
       "      <td>The user is asking how to use Atlan's API/SDK ...</td>\n",
       "      <td>True</td>\n",
       "      <td>API/SDK, How-to</td>\n",
       "      <td>webhooks, Atlan, Slack, API</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TICKET-268</td>\n",
       "      <td>Triggering an AWS Lambda from Atlan events</td>\n",
       "      <td>We have a workflow where we want to trigger a ...</td>\n",
       "      <td>[TopicTag.API_SDK, TopicTag.SENSITIVE_DATA]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan tag, AWS Lambda, API Gateway, webhooks]</td>\n",
       "      <td>The user is asking for a how-to on integrating...</td>\n",
       "      <td>True</td>\n",
       "      <td>API/SDK, Sensitive data</td>\n",
       "      <td>Atlan tag, AWS Lambda, API Gateway, webhooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TICKET-269</td>\n",
       "      <td>When to use Atlan automations vs. external ser...</td>\n",
       "      <td>I see that Atlan has a built-in 'Automations' ...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.BEST_PRACTICES]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, Automations, Zapier, Airflow]</td>\n",
       "      <td>The user is asking for guidance on how to use ...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Best practices</td>\n",
       "      <td>Atlan, Automations, Zapier, Airflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TICKET-270</td>\n",
       "      <td>Connector failed to crawl - where to check logs?</td>\n",
       "      <td>URGENT: Our nightly Snowflake crawler failed l...</td>\n",
       "      <td>[TopicTag.PRODUCT, TopicTag.CONNECTOR, TopicTa...</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0.95</td>\n",
       "      <td>P0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Snowflake, crawler, metadata, lineage]</td>\n",
       "      <td>The ticket reports a critical failure with the...</td>\n",
       "      <td>True</td>\n",
       "      <td>Product, Connector, Lineage</td>\n",
       "      <td>Snowflake, crawler, metadata, lineage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TICKET-271</td>\n",
       "      <td>Asset extracted but not published to Atlan</td>\n",
       "      <td>This is very strange. I'm looking at the crawl...</td>\n",
       "      <td>[TopicTag.PRODUCT, TopicTag.CONNECTOR]</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>0.60</td>\n",
       "      <td>P1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>[crawler logs, schema.my_table, Atlan UI]</td>\n",
       "      <td>The user is experiencing an issue where an ass...</td>\n",
       "      <td>True</td>\n",
       "      <td>Product, Connector</td>\n",
       "      <td>crawler logs, schema.my_table, Atlan UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TICKET-272</td>\n",
       "      <td>How to measure adoption and generate reports?</td>\n",
       "      <td>My manager is asking for metrics on our Atlan ...</td>\n",
       "      <td>[TopicTag.HOW_TO]</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.30</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan usage, metrics, report]</td>\n",
       "      <td>The user is asking a 'how-to' question about g...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to</td>\n",
       "      <td>Atlan usage, metrics, report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TICKET-273</td>\n",
       "      <td>Best practices for catalog hygiene</td>\n",
       "      <td>We've been using Atlan for six months, and our...</td>\n",
       "      <td>[TopicTag.BEST_PRACTICES, TopicTag.GLOSSARY]</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, metadata]</td>\n",
       "      <td>The user is asking for best practices to maint...</td>\n",
       "      <td>True</td>\n",
       "      <td>Best practices, Glossary</td>\n",
       "      <td>Atlan, metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TICKET-274</td>\n",
       "      <td>How to scale Atlan across multiple business un...</td>\n",
       "      <td>We are planning a global rollout of Atlan to m...</td>\n",
       "      <td>[TopicTag.HOW_TO, TopicTag.BEST_PRACTICES, Top...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Curious</td>\n",
       "      <td>0.20</td>\n",
       "      <td>P2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>[Atlan, workspaces, teams, permissions, govern...</td>\n",
       "      <td>The user is asking for advice on how to struct...</td>\n",
       "      <td>True</td>\n",
       "      <td>How-to, Best practices, Other</td>\n",
       "      <td>Atlan, workspaces, teams, permissions, governance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticket_id                                            subject  \\\n",
       "0   TICKET-245  Connecting Snowflake to Atlan - required permi...   \n",
       "1   TICKET-246    Which connectors automatically capture lineage?   \n",
       "2   TICKET-247    Deployment of Atlan agent for private data lake   \n",
       "3   TICKET-248     How to surface sample rows and schema changes?   \n",
       "4   TICKET-249        Exporting lineage view for a specific table   \n",
       "5   TICKET-250                Importing lineage from Airflow jobs   \n",
       "6   TICKET-251                     Using the Visual Query Builder   \n",
       "7   TICKET-252                 Programmatic extraction of lineage   \n",
       "8   TICKET-253     Upstream lineage to Snowflake view not working   \n",
       "9   TICKET-254  How to create a business glossary and link ter...   \n",
       "10  TICKET-255           Creating a custom role for data stewards   \n",
       "11  TICKET-256     Mapping Active Directory groups to Atlan teams   \n",
       "12  TICKET-257                     RBAC for assets vs. glossaries   \n",
       "13  TICKET-258                Process for onboarding asset owners   \n",
       "14  TICKET-259  How does Atlan surface sensitive fields like PII?   \n",
       "15  TICKET-260           Authentication methods for APIs and SDKs   \n",
       "16  TICKET-261                      Enabling and testing SAML SSO   \n",
       "17  TICKET-262      SSO login not assigning user to correct group   \n",
       "18  TICKET-263   Integration with existing DLP or secrets manager   \n",
       "19  TICKET-264        Accessing audit logs for compliance reviews   \n",
       "20  TICKET-265  How to programmatically create an asset using ...   \n",
       "21  TICKET-266                SDK availability and Python example   \n",
       "22  TICKET-267                     How do webhooks work in Atlan?   \n",
       "23  TICKET-268         Triggering an AWS Lambda from Atlan events   \n",
       "24  TICKET-269  When to use Atlan automations vs. external ser...   \n",
       "25  TICKET-270   Connector failed to crawl - where to check logs?   \n",
       "26  TICKET-271         Asset extracted but not published to Atlan   \n",
       "27  TICKET-272      How to measure adoption and generate reports?   \n",
       "28  TICKET-273                 Best practices for catalog hygiene   \n",
       "29  TICKET-274  How to scale Atlan across multiple business un...   \n",
       "\n",
       "                                                 body  \\\n",
       "0   Hi team, we're trying to set up our primary Sn...   \n",
       "1   Hello, I'm new to Atlan and trying to understa...   \n",
       "2   Our primary data lake is hosted on-premise wit...   \n",
       "3   Hi, we've successfully connected our Redshift ...   \n",
       "4   For our quarterly audit, I need to provide a c...   \n",
       "5   We run hundreds of ETL jobs in Airflow, and we...   \n",
       "6   I'm a business analyst and not very comfortabl...   \n",
       "7   Our internal data science team wants to build ...   \n",
       "8   This is infuriating. We have a critical Snowfl...   \n",
       "9   We are migrating our existing business glossar...   \n",
       "10  I'm trying to set up a custom role for our dat...   \n",
       "11  Our company policy requires us to manage all u...   \n",
       "12  I need clarification on how Atlan's role-based...   \n",
       "13  We've started identifying owners for our key d...   \n",
       "14  Our security team is evaluating Atlan and thei...   \n",
       "15  We are planning to build several automations u...   \n",
       "16  We are ready to enable SAML SSO with our Okta ...   \n",
       "17  I've just had a new user, 'test.user@company.c...   \n",
       "18  Does Atlan have the capability to integrate wi...   \n",
       "19  Our compliance team needs to perform a quarter...   \n",
       "20  I'm trying to create a new custom asset (a 'Re...   \n",
       "21  I'm a data engineer and prefer using SDKs over...   \n",
       "22  I'm exploring using webhooks to send real-time...   \n",
       "23  We have a workflow where we want to trigger a ...   \n",
       "24  I see that Atlan has a built-in 'Automations' ...   \n",
       "25  URGENT: Our nightly Snowflake crawler failed l...   \n",
       "26  This is very strange. I'm looking at the crawl...   \n",
       "27  My manager is asking for metrics on our Atlan ...   \n",
       "28  We've been using Atlan for six months, and our...   \n",
       "29  We are planning a global rollout of Atlan to m...   \n",
       "\n",
       "                                               topics  sentiment_score  \\\n",
       "0               [TopicTag.CONNECTOR, TopicTag.HOW_TO]             -0.4   \n",
       "1   [TopicTag.HOW_TO, TopicTag.CONNECTOR, TopicTag...              0.4   \n",
       "2               [TopicTag.HOW_TO, TopicTag.CONNECTOR]             -0.5   \n",
       "3               [TopicTag.HOW_TO, TopicTag.CONNECTOR]              0.4   \n",
       "4                 [TopicTag.LINEAGE, TopicTag.HOW_TO]             -0.5   \n",
       "5   [TopicTag.HOW_TO, TopicTag.CONNECTOR, TopicTag...              0.3   \n",
       "6                                   [TopicTag.HOW_TO]              0.4   \n",
       "7                [TopicTag.LINEAGE, TopicTag.API_SDK]              0.3   \n",
       "8   [TopicTag.PRODUCT, TopicTag.CONNECTOR, TopicTa...             -0.8   \n",
       "9   [TopicTag.GLOSSARY, TopicTag.API_SDK, TopicTag...             -0.5   \n",
       "10               [TopicTag.HOW_TO, TopicTag.GLOSSARY]              0.3   \n",
       "11                    [TopicTag.HOW_TO, TopicTag.SSO]              0.4   \n",
       "12  [TopicTag.HOW_TO, TopicTag.GLOSSARY, TopicTag....              0.4   \n",
       "13               [TopicTag.HOW_TO, TopicTag.GLOSSARY]              0.4   \n",
       "14         [TopicTag.SENSITIVE_DATA, TopicTag.HOW_TO]              0.4   \n",
       "15                [TopicTag.API_SDK, TopicTag.HOW_TO]              0.4   \n",
       "16                    [TopicTag.SSO, TopicTag.HOW_TO]              0.4   \n",
       "17                   [TopicTag.SSO, TopicTag.PRODUCT]             -0.4   \n",
       "18      [TopicTag.CONNECTOR, TopicTag.SENSITIVE_DATA]              0.3   \n",
       "19  [TopicTag.HOW_TO, TopicTag.API_SDK, TopicTag.S...              0.4   \n",
       "20                [TopicTag.HOW_TO, TopicTag.API_SDK]              0.4   \n",
       "21  [TopicTag.API_SDK, TopicTag.HOW_TO, TopicTag.G...              0.4   \n",
       "22                [TopicTag.API_SDK, TopicTag.HOW_TO]              0.4   \n",
       "23        [TopicTag.API_SDK, TopicTag.SENSITIVE_DATA]              0.4   \n",
       "24         [TopicTag.HOW_TO, TopicTag.BEST_PRACTICES]              0.4   \n",
       "25  [TopicTag.PRODUCT, TopicTag.CONNECTOR, TopicTa...             -0.7   \n",
       "26             [TopicTag.PRODUCT, TopicTag.CONNECTOR]             -0.4   \n",
       "27                                  [TopicTag.HOW_TO]              0.4   \n",
       "28       [TopicTag.BEST_PRACTICES, TopicTag.GLOSSARY]              0.3   \n",
       "29  [TopicTag.HOW_TO, TopicTag.BEST_PRACTICES, Top...              0.3   \n",
       "\n",
       "   sentiment_label  urgency_score priority  confidence  \\\n",
       "0       Frustrated           0.90       P0        0.95   \n",
       "1          Curious           0.70       P1        0.80   \n",
       "2       Frustrated           0.90       P0        0.90   \n",
       "3          Curious           0.30       P2        0.90   \n",
       "4       Frustrated           0.80       P1        0.90   \n",
       "5          Curious           0.30       P2        0.80   \n",
       "6          Curious           0.20       P2        0.90   \n",
       "7          Curious           0.20       P2        0.90   \n",
       "8            Angry           0.95       P0        0.90   \n",
       "9       Frustrated           0.80       P1        0.90   \n",
       "10         Curious           0.30       P2        0.90   \n",
       "11         Curious           0.20       P2        0.80   \n",
       "12         Curious           0.30       P2        0.90   \n",
       "13         Curious           0.20       P2        0.90   \n",
       "14         Curious           0.30       P2        0.90   \n",
       "15         Curious           0.30       P2        0.90   \n",
       "16         Curious           0.60       P1        0.90   \n",
       "17      Frustrated           0.70       P1        0.90   \n",
       "18         Curious           0.20       P2        0.90   \n",
       "19         Curious           0.30       P2        0.90   \n",
       "20         Curious           0.20       P2        0.90   \n",
       "21         Curious           0.20       P2        0.90   \n",
       "22         Curious           0.20       P2        0.90   \n",
       "23         Curious           0.30       P2        0.90   \n",
       "24         Curious           0.20       P2        0.90   \n",
       "25           Angry           0.95       P0        0.90   \n",
       "26      Frustrated           0.60       P1        0.80   \n",
       "27         Curious           0.30       P2        0.90   \n",
       "28         Curious           0.20       P2        0.90   \n",
       "29         Curious           0.20       P2        0.90   \n",
       "\n",
       "                                         key_entities  \\\n",
       "0                         [Snowflake, Atlan, BI team]   \n",
       "1                            [Fivetran, dbt, Tableau]   \n",
       "2                                  [Atlan agent, VPC]   \n",
       "3                                          [Redshift]   \n",
       "4                [fact_orders table, lineage diagram]   \n",
       "5                      [Airflow, ETL, DAGs, datasets]   \n",
       "6                         [Visual Query Builder, SQL]   \n",
       "7                                      [API, lineage]   \n",
       "8                       [Snowflake, lineage, crawler]   \n",
       "9                       [Atlan, CSV, API, Governance]   \n",
       "10  [custom role, data stewards, permissions, glos...   \n",
       "11  [Active Directory, AD groups, Atlan teams, per...   \n",
       "12  [Atlan, role-based access control, Snowflake, ...   \n",
       "13        [owners, data assets, Atlan, notifications]   \n",
       "14                                       [PII, Atlan]   \n",
       "15           [Atlan API, Python SDK, OAuth, API keys]   \n",
       "16                                  [SAML, SSO, Okta]   \n",
       "17                      [SSO, SAML, group assignment]   \n",
       "18     [Atlan, DLP, HashiCorp Vault, secrets manager]   \n",
       "19                                  [audit logs, API]   \n",
       "20                   [REST API, custom asset, Report]   \n",
       "21            [SDK, API, Python, PyPI, glossary term]   \n",
       "22                      [webhooks, Atlan, Slack, API]   \n",
       "23     [Atlan tag, AWS Lambda, API Gateway, webhooks]   \n",
       "24              [Atlan, Automations, Zapier, Airflow]   \n",
       "25            [Snowflake, crawler, metadata, lineage]   \n",
       "26          [crawler logs, schema.my_table, Atlan UI]   \n",
       "27                     [Atlan usage, metrics, report]   \n",
       "28                                  [Atlan, metadata]   \n",
       "29  [Atlan, workspaces, teams, permissions, govern...   \n",
       "\n",
       "                                            reasoning  classification_success  \\\n",
       "0   The user is experiencing a connection failure ...                    True   \n",
       "1   The user is asking for a 'how-to' on lineage c...                    True   \n",
       "2   The user is asking for help with setting up th...                    True   \n",
       "3   The user is asking how to perform a task ('how...                    True   \n",
       "4   The user needs to export lineage information f...                    True   \n",
       "5   The user is asking a 'how-to' question about i...                    True   \n",
       "6   The user is asking for instructions on how to ...                    True   \n",
       "7   The user is asking how to programmatically ext...                    True   \n",
       "8   The user expresses strong dissatisfaction ('in...                    True   \n",
       "9   The user is asking how to bulk import glossary...                    True   \n",
       "10  The user is asking a 'how-to' question about s...                    True   \n",
       "11  The user is asking how to configure Atlan to m...                    True   \n",
       "12  The user is asking a 'how-to' question about A...                    True   \n",
       "13  The user is asking how to perform a task in At...                    True   \n",
       "14  The user is asking 'how-to' questions about id...                    True   \n",
       "15  The user is asking \"how-to\" questions about us...                    True   \n",
       "16  The user is asking how to test an SSO configur...                    True   \n",
       "17  The user is experiencing an issue with SSO gro...                    True   \n",
       "18  The user is asking about integration capabilit...                    True   \n",
       "19  The user is asking how to access and export au...                    True   \n",
       "20  The user is asking for guidance on how to use ...                    True   \n",
       "21  The user is asking a 'how-to' question about S...                    True   \n",
       "22  The user is asking how to use Atlan's API/SDK ...                    True   \n",
       "23  The user is asking for a how-to on integrating...                    True   \n",
       "24  The user is asking for guidance on how to use ...                    True   \n",
       "25  The ticket reports a critical failure with the...                    True   \n",
       "26  The user is experiencing an issue where an ass...                    True   \n",
       "27  The user is asking a 'how-to' question about g...                    True   \n",
       "28  The user is asking for best practices to maint...                    True   \n",
       "29  The user is asking for advice on how to struct...                    True   \n",
       "\n",
       "                          topics_str  \\\n",
       "0                  Connector, How-to   \n",
       "1         How-to, Connector, Lineage   \n",
       "2                  How-to, Connector   \n",
       "3                  How-to, Connector   \n",
       "4                    Lineage, How-to   \n",
       "5         How-to, Connector, Lineage   \n",
       "6                             How-to   \n",
       "7                   Lineage, API/SDK   \n",
       "8        Product, Connector, Lineage   \n",
       "9          Glossary, API/SDK, How-to   \n",
       "10                  How-to, Glossary   \n",
       "11                       How-to, SSO   \n",
       "12  How-to, Glossary, Sensitive data   \n",
       "13                  How-to, Glossary   \n",
       "14            Sensitive data, How-to   \n",
       "15                   API/SDK, How-to   \n",
       "16                       SSO, How-to   \n",
       "17                      SSO, Product   \n",
       "18         Connector, Sensitive data   \n",
       "19   How-to, API/SDK, Sensitive data   \n",
       "20                   How-to, API/SDK   \n",
       "21         API/SDK, How-to, Glossary   \n",
       "22                   API/SDK, How-to   \n",
       "23           API/SDK, Sensitive data   \n",
       "24            How-to, Best practices   \n",
       "25       Product, Connector, Lineage   \n",
       "26                Product, Connector   \n",
       "27                            How-to   \n",
       "28          Best practices, Glossary   \n",
       "29     How-to, Best practices, Other   \n",
       "\n",
       "                                     key_entities_str  \n",
       "0                           Snowflake, Atlan, BI team  \n",
       "1                              Fivetran, dbt, Tableau  \n",
       "2                                    Atlan agent, VPC  \n",
       "3                                            Redshift  \n",
       "4                  fact_orders table, lineage diagram  \n",
       "5                        Airflow, ETL, DAGs, datasets  \n",
       "6                           Visual Query Builder, SQL  \n",
       "7                                        API, lineage  \n",
       "8                         Snowflake, lineage, crawler  \n",
       "9                         Atlan, CSV, API, Governance  \n",
       "10  custom role, data stewards, permissions, gloss...  \n",
       "11  Active Directory, AD groups, Atlan teams, perm...  \n",
       "12  Atlan, role-based access control, Snowflake, g...  \n",
       "13          owners, data assets, Atlan, notifications  \n",
       "14                                         PII, Atlan  \n",
       "15             Atlan API, Python SDK, OAuth, API keys  \n",
       "16                                    SAML, SSO, Okta  \n",
       "17                        SSO, SAML, group assignment  \n",
       "18       Atlan, DLP, HashiCorp Vault, secrets manager  \n",
       "19                                    audit logs, API  \n",
       "20                     REST API, custom asset, Report  \n",
       "21              SDK, API, Python, PyPI, glossary term  \n",
       "22                        webhooks, Atlan, Slack, API  \n",
       "23       Atlan tag, AWS Lambda, API Gateway, webhooks  \n",
       "24                Atlan, Automations, Zapier, Airflow  \n",
       "25              Snowflake, crawler, metadata, lineage  \n",
       "26            crawler logs, schema.my_table, Atlan UI  \n",
       "27                       Atlan usage, metrics, report  \n",
       "28                                    Atlan, metadata  \n",
       "29  Atlan, workspaces, teams, permissions, governance  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, Any, Optional\n",
    "import time\n",
    "\n",
    "def classify_ticket(ticket: Dict[str, Any], client: genai.Client) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Classify a single ticket using Gemini API\n",
    "    \n",
    "    Args:\n",
    "        ticket: Dictionary containing ticket data with 'id', 'subject', and 'body'\n",
    "        client: Gemini API client\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing ticket classification results\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an AI triage assistant for a data platform's support team.\n",
    "Your task: analyze a support ticket and output only valid JSON with the required fields.\n",
    "\n",
    "Rules\n",
    "\n",
    "Topics (multi-label):\n",
    "Select all that apply from this list:\n",
    "\"How-to\": user asks how to use a feature or complete a task.\n",
    "\"Product\": bug, error, or unexpected behavior in Atlan.\n",
    "\"Connector\": issues connecting/integrating external systems (Snowflake, Redshift, BI tools, etc.).\n",
    "\"Lineage\": lineage diagrams, capture, missing lineage.\n",
    "\"API/SDK\": APIs, SDKs, webhooks, programmatic access.\n",
    "\"SSO\": authentication, login, SSO, identity providers.\n",
    "\"Glossary\": glossaries, business terms, linking.\n",
    "\"Best practices\": recommendations, workflows, catalog hygiene.\n",
    "\"Sensitive data\": PII, data masking, compliance.\n",
    "\"Other\": if none fit.\n",
    "\n",
    "Urgency & Priority:\n",
    "Mentions of urgent, blocked, deadline, critical failure → priority=\"P0\", urgency_score≈0.9–1.0.\n",
    "Important but not blocking → priority=\"P1\", urgency_score≈0.5–0.8.\n",
    "Informational/low urgency → priority=\"P2\", urgency_score≈0.1–0.4.\n",
    "\n",
    "Sentiment:\n",
    "\"Frustrated\": blocked, struggling, urgency, mild negativity → sentiment_score≈-0.3 to -0.6.\n",
    "\"Angry\": strong dissatisfaction or infuriated → sentiment_score≈-0.7 to -1.0.\n",
    "\"Curious\": exploring, asking questions, polite → sentiment_score≈0.2 to 0.6.\n",
    "\"Neutral\": factual or polite without emotion → sentiment_score≈-0.1 to 0.1.\n",
    "\n",
    "Other fields:\n",
    "key_entities: short technical terms (e.g., \"Snowflake\", \"dbt\", \"Okta\"), not full sentences.\n",
    "reasoning: 1–2 sentences explaining why you chose these labels.\n",
    "confidence: 0.0–1.0 (higher if ticket is clear).\n",
    "\n",
    "=== CLASSIFY THIS TICKET ===\n",
    "{ticket['body']}\n",
    "=== END TICKET ===\n",
    "\n",
    "Return only JSON.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash-lite',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json',\n",
    "                'response_schema': TicketFeatures,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        features = response.parsed\n",
    "        \n",
    "        result = {\n",
    "            'ticket_id': ticket['id'],\n",
    "            'subject': ticket['subject'],\n",
    "            'body': ticket['body'],\n",
    "            'topics': features.topics if features else [],\n",
    "            'sentiment_score': features.sentiment_score if features else None,\n",
    "            'sentiment_label': features.sentiment_label if features else None,\n",
    "            'urgency_score': features.urgency_score if features else None,\n",
    "            'priority': features.priority if features else None,\n",
    "            'confidence': features.confidence if features else None,\n",
    "            'key_entities': features.key_entities if features else [],\n",
    "            'reasoning': features.reasoning if features else None,\n",
    "            'classification_success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying ticket {ticket['id']}: {str(e)}\")\n",
    "        result = {\n",
    "            'ticket_id': ticket['id'],\n",
    "            'subject': ticket['subject'],\n",
    "            'body': ticket['body'],\n",
    "            'topics': [],\n",
    "            'sentiment_score': None,\n",
    "            'sentiment_label': None,\n",
    "            'urgency_score': None,\n",
    "            'priority': None,\n",
    "            'confidence': None,\n",
    "            'key_entities': [],\n",
    "            'reasoning': None,\n",
    "            'classification_success': False\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_tickets_with_pause(tickets: list, client: genai.Client, batch_size: int = 15, pause_seconds: int = 60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process tickets in batches, pausing between batches to avoid rate limits.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(tickets)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        print(f\"Processing tickets {start+1} to {end} of {total}\")\n",
    "        for i in range(start, end):\n",
    "            result = classify_ticket(tickets[i], client)\n",
    "            results.append(result)\n",
    "        if end < total:\n",
    "            print(f\"Batch complete. Waiting {pause_seconds} seconds before next batch...\")\n",
    "            time.sleep(pause_seconds)\n",
    "    df = pd.DataFrame(results)\n",
    "    df['topics_str'] = df['topics'].apply(lambda x: ', '.join(x) if x else '')\n",
    "    df['key_entities_str'] = df['key_entities'].apply(lambda x: ', '.join(x) if x else '')\n",
    "    print(f\"Successfully processed {len(df)} tickets\")\n",
    "    return df\n",
    "\n",
    "# Run the new batch-processing function for all tickets\n",
    "results_df = process_tickets_with_pause(tickets, client, batch_size=15, pause_seconds=60)\n",
    "\n",
    "# Remove all rows with any NaN values\n",
    "results_df_clean = results_df.dropna()\n",
    "\n",
    "# Save cleaned DataFrame to CSV\n",
    "results_df_clean.to_csv('Ticket_Classification_Results_CLEAN.csv', index=False)\n",
    "print(\"Saved cleaned results to Ticket_Classification_Results_CLEAN.csv\")\n",
    "results_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63767d",
   "metadata": {},
   "source": [
    "Topic Evaluation\n",
    "- Bucket tickets by topic\n",
    "- Compute bucket coherence (Similarity of imp sentences in texts (using sentence salience))\n",
    "- Compute agreement rate\n",
    "- Compute stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e15e442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting salient sentences...\n",
      "Computing embeddings...\n",
      "Computing embeddings...\n"
     ]
    }
   ],
   "source": [
    "# MULTI-LABEL EVALUATION WITH SENTENCE SALIENCE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download punkt if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class MultiLabelEvaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def parse_topics(self, topics_field):\n",
    "        if isinstance(topics_field, list):\n",
    "            return [str(t).replace('TopicTag.', '') for t in topics_field]\n",
    "        if isinstance(topics_field, str):\n",
    "            if 'TopicTag.' in topics_field:\n",
    "                import re\n",
    "                pattern = r\"'([^']+)'\"\n",
    "                matches = re.findall(pattern, topics_field)\n",
    "                return matches\n",
    "            return [t.strip() for t in topics_field.split(',') if t.strip()]\n",
    "        return []\n",
    "    \n",
    "    def extract_salient_sentences(self, text, top_k=2):\n",
    "        \"\"\"Extract most salient sentences based on embedding similarity to full text\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) <= top_k:\n",
    "            return sentences\n",
    "        \n",
    "        # Get embeddings\n",
    "        full_text_emb = self.sentence_model.encode([text])\n",
    "        sentence_embs = self.sentence_model.encode(sentences)\n",
    "        \n",
    "        # Calculate salience (similarity to full text)\n",
    "        similarities = cosine_similarity(sentence_embs, full_text_emb).flatten()\n",
    "        \n",
    "        # Get top-k most salient sentences\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        return [sentences[i] for i in sorted(top_indices)]\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # Parse topics\n",
    "        self.df['topics_parsed'] = self.df['topics'].apply(self.parse_topics)\n",
    "        \n",
    "        # Extract salient sentences for each ticket\n",
    "        print(\"Extracting salient sentences...\")\n",
    "        salient_texts = []\n",
    "        for text in self.df['body']:\n",
    "            salient_sentences = self.extract_salient_sentences(text, top_k=2)\n",
    "            salient_texts.append(' '.join(salient_sentences))\n",
    "        \n",
    "        # Get embeddings of salient text only\n",
    "        print(\"Computing embeddings...\")\n",
    "        text_embeddings = self.sentence_model.encode(salient_texts)\n",
    "        \n",
    "        # Build similarity graph\n",
    "        similarity_matrix = cosine_similarity(text_embeddings)\n",
    "        G = nx.Graph()\n",
    "        similarity_threshold = 0.3  \n",
    "        \n",
    "        # Add nodes\n",
    "        for i in range(len(self.df)):\n",
    "            G.add_node(i, ticket_id=self.df.iloc[i]['ticket_id'])\n",
    "        \n",
    "        # Add edges for similar tickets\n",
    "        for i in range(len(self.df)):\n",
    "            for j in range(i+1, len(self.df)):\n",
    "                if similarity_matrix[i][j] > similarity_threshold:\n",
    "                    G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "        \n",
    "        # Find clusters\n",
    "        clusters = list(nx.connected_components(G))\n",
    "        cluster_analysis = {}\n",
    "        outliers = []\n",
    "        \n",
    "        for cluster_id, cluster in enumerate(clusters):\n",
    "            if len(cluster) == 1:\n",
    "                node = list(cluster)[0]\n",
    "                outliers.append(self.df.iloc[node]['ticket_id'])\n",
    "                continue\n",
    "                \n",
    "            # Analyze cluster topics\n",
    "            cluster_topics = []\n",
    "            for node in cluster:\n",
    "                cluster_topics.extend(self.df.iloc[node]['topics_parsed'])\n",
    "            \n",
    "            topic_counter = Counter(cluster_topics)\n",
    "            \n",
    "            # Calculate coherence using salient embeddings\n",
    "            cluster_embeddings = text_embeddings[list(cluster)]\n",
    "            cluster_sim_matrix = cosine_similarity(cluster_embeddings)\n",
    "            upper_triangle = cluster_sim_matrix[np.triu_indices_from(cluster_sim_matrix, k=1)]\n",
    "            avg_coherence = np.mean(upper_triangle) if len(upper_triangle) > 0 else 0.0\n",
    "            \n",
    "            cluster_analysis[cluster_id] = {\n",
    "                'size': len(cluster),\n",
    "                'coherence': avg_coherence,\n",
    "                'dominant_topics': topic_counter.most_common(3),\n",
    "                'tickets': [self.df.iloc[node]['ticket_id'] for node in cluster]\n",
    "            }\n",
    "        \n",
    "        return cluster_analysis, outliers, similarity_matrix\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = MultiLabelEvaluator(results_df_clean)\n",
    "cluster_analysis, outliers, similarity_matrix = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68616f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity clusters found: 1\n",
      "Outliers (isolated tickets): 1\n",
      "Outlier rate: 3.3%\n",
      "\n",
      "Cluster Analysis:\n",
      "Cluster 0: 29 tickets, coherence: 0.226\n",
      "  Topics: HOW_TO(21) + CONNECTOR(9)\n",
      "  Tickets: TICKET-245, TICKET-246, TICKET-247...\n",
      "\n",
      "Outlier tickets: TICKET-265\n",
      "\n",
      "Overall average similarity: 0.214\n"
     ]
    }
   ],
   "source": [
    "# RESULTS - GRAPH-BASED ANALYSIS\n",
    "print(f\"Similarity clusters found: {len(cluster_analysis)}\")\n",
    "print(f\"Outliers (isolated tickets): {len(outliers)}\")\n",
    "print(f\"Outlier rate: {len(outliers)/len(results_df_clean)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCluster Analysis:\")\n",
    "for cluster_id, data in cluster_analysis.items():\n",
    "    topics_str = ' + '.join([f\"{topic}({count})\" for topic, count in data['dominant_topics'][:2]])\n",
    "    print(f\"Cluster {cluster_id}: {data['size']} tickets, coherence: {data['coherence']:.3f}\")\n",
    "    print(f\"  Topics: {topics_str}\")\n",
    "    print(f\"  Tickets: {', '.join(data['tickets'][:3])}{'...' if len(data['tickets']) > 3 else ''}\")\n",
    "\n",
    "if outliers:\n",
    "    print(f\"\\nOutlier tickets: {', '.join(outliers)}\")\n",
    "\n",
    "# Overall similarity stats\n",
    "avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "print(f\"\\nOverall average similarity: {avg_similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f19e7fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION SUMMARY ===\n",
      "Total tickets: 30\n",
      "Status breakdown: {'confident': 10, 'moderate': 13, 'disagreement': 6, 'isolated': 1}\n",
      "Actions needed: {'keep': 24, 'review': 6}\n",
      "\n",
      "Cluster quality:\n",
      "  Average coherence: 0.226\n",
      "  Coherence range: 0.226 - 0.226\n",
      "\n",
      "Review priorities:\n",
      "  0 tickets need manual review\n",
      "  6 tickets need quick review\n"
     ]
    }
   ],
   "source": [
    "# CONSENSUS VALIDATION - CLEAN & LOGICAL\n",
    "def validate_labels(df, cluster_analysis, outliers):\n",
    "    \"\"\"Validate labels using cluster consensus - lean logic\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Map tickets to clusters\n",
    "    ticket_to_cluster = {}\n",
    "    for cluster_id, data in cluster_analysis.items():\n",
    "        for ticket_id in data['tickets']:\n",
    "            ticket_to_cluster[ticket_id] = cluster_id\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        ticket_id = row['ticket_id']\n",
    "        current_topics = set(evaluator.parse_topics(row['topics']))\n",
    "        \n",
    "        if ticket_id in outliers:\n",
    "            # Don't automatically mark outliers as needing manual review\n",
    "            # Many outliers might still have correct labels\n",
    "            results.append({\n",
    "                'ticket_id': ticket_id,\n",
    "                'status': 'isolated',\n",
    "                'confidence': 'medium',  # Changed from 'low'\n",
    "                'action': 'keep'  # Changed from 'manual_review' - trust the original classification\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get cluster consensus\n",
    "        cluster_id = ticket_to_cluster[ticket_id]\n",
    "        cluster_data = cluster_analysis[cluster_id]\n",
    "        \n",
    "        # Calculate topic agreement\n",
    "        cluster_topics = dict(cluster_data['dominant_topics'])\n",
    "        cluster_topic_set = set(cluster_topics.keys())\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        intersection = len(current_topics & cluster_topic_set)\n",
    "        union = len(current_topics | cluster_topic_set)\n",
    "        agreement = intersection / union if union > 0 else 0\n",
    "        \n",
    "        # More realistic thresholds aligned with 90% ground truth accuracy\n",
    "        if agreement >= 0.5:  # Relaxed from 0.7\n",
    "            status = 'confident'\n",
    "            confidence = 'high'\n",
    "            action = 'keep'\n",
    "        elif agreement >= 0.25:  # Relaxed from 0.4\n",
    "            status = 'moderate'\n",
    "            confidence = 'medium'\n",
    "            action = 'keep'  # Changed from 'review' - most should be kept\n",
    "        else:\n",
    "            status = 'disagreement'\n",
    "            confidence = 'low'\n",
    "            action = 'review'  # Changed from 'manual_review'\n",
    "        \n",
    "        results.append({\n",
    "            'ticket_id': ticket_id,\n",
    "            'status': status,\n",
    "            'confidence': confidence,\n",
    "            'action': action,\n",
    "            'agreement_score': agreement,\n",
    "            'cluster_coherence': cluster_data['coherence']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_labels(results_df_clean, cluster_analysis, outliers)\n",
    "\n",
    "# Clean summary\n",
    "status_counts = Counter([r['status'] for r in validation_results])\n",
    "action_counts = Counter([r['action'] for r in validation_results])\n",
    "\n",
    "print(\"=== VALIDATION SUMMARY ===\")\n",
    "print(f\"Total tickets: {len(validation_results)}\")\n",
    "print(f\"Status breakdown: {dict(status_counts)}\")\n",
    "print(f\"Actions needed: {dict(action_counts)}\")\n",
    "\n",
    "print(f\"\\nCluster quality:\")\n",
    "if cluster_analysis:\n",
    "    coherences = [data['coherence'] for data in cluster_analysis.values()]\n",
    "    print(f\"  Average coherence: {np.mean(coherences):.3f}\")\n",
    "    print(f\"  Coherence range: {min(coherences):.3f} - {max(coherences):.3f}\")\n",
    "\n",
    "print(f\"\\nReview priorities:\")\n",
    "manual_review = [r for r in validation_results if r['action'] == 'manual_review']\n",
    "print(f\"  {len(manual_review)} tickets need manual review\")\n",
    "\n",
    "review_needed = [r for r in validation_results if r['action'] == 'review']\n",
    "print(f\"  {len(review_needed)} tickets need quick review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e64da0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-processing 6 tickets flagged for review...\n",
      "Found 6 tickets to re-process\n",
      "Re-classifying TICKET-253...\n",
      "Re-classifying TICKET-257...\n",
      "Re-classifying TICKET-262...\n",
      "Re-classifying TICKET-270...\n",
      "Re-classifying TICKET-273...\n",
      "Re-classifying TICKET-274...\n",
      "\n",
      "=== RE-CLASSIFICATION RESULTS ===\n",
      "\n",
      "TICKET-253:\n",
      "  Original: {'CONNECTOR', 'PRODUCT', 'LINEAGE'}\n",
      "  New:      {'CONNECTOR', 'PRODUCT', 'LINEAGE'}\n",
      "  Changed:  No\n",
      "\n",
      "TICKET-257:\n",
      "  Original: {'GLOSSARY', 'SENSITIVE_DATA', 'HOW_TO'}\n",
      "  New:      {'GLOSSARY', 'SENSITIVE_DATA', 'HOW_TO'}\n",
      "  Changed:  No\n",
      "\n",
      "TICKET-262:\n",
      "  Original: {'SSO', 'PRODUCT'}\n",
      "  New:      {'SSO', 'PRODUCT'}\n",
      "  Changed:  No\n",
      "\n",
      "TICKET-270:\n",
      "  Original: {'CONNECTOR', 'PRODUCT', 'LINEAGE'}\n",
      "  New:      {'CONNECTOR', 'PRODUCT', 'LINEAGE'}\n",
      "  Changed:  No\n",
      "\n",
      "TICKET-273:\n",
      "  Original: {'GLOSSARY', 'BEST_PRACTICES'}\n",
      "  New:      {'BEST_PRACTICES'}\n",
      "  Changed:  Yes\n",
      "\n",
      "TICKET-274:\n",
      "  Original: {'OTHER', 'BEST_PRACTICES', 'HOW_TO'}\n",
      "  New:      {'BEST_PRACTICES', 'HOW_TO'}\n",
      "  Changed:  Yes\n",
      "\n",
      "Saved re-classification results to Review_Tickets_Reclassified.csv\n"
     ]
    }
   ],
   "source": [
    "# RE-PROCESS REVIEW TICKETS\n",
    "review_tickets = [r for r in validation_results if r['action'] == 'review']\n",
    "print(f\"Re-processing {len(review_tickets)} tickets flagged for review...\")\n",
    "\n",
    "if review_tickets:\n",
    "    # Get the original ticket data for review tickets\n",
    "    review_ticket_ids = [r['ticket_id'] for r in review_tickets]\n",
    "    review_ticket_data = []\n",
    "    \n",
    "    for ticket in tickets:\n",
    "        if ticket['id'] in review_ticket_ids:\n",
    "            review_ticket_data.append(ticket)\n",
    "    \n",
    "    print(f\"Found {len(review_ticket_data)} tickets to re-process\")\n",
    "    \n",
    "    # Re-run classification on review tickets\n",
    "    review_results = []\n",
    "    for ticket in review_ticket_data:\n",
    "        print(f\"Re-classifying {ticket['id']}...\")\n",
    "        result = classify_ticket(ticket, client)\n",
    "        review_results.append(result)\n",
    "    \n",
    "    # Create DataFrame for review results\n",
    "    review_df = pd.DataFrame(review_results)\n",
    "    review_df['topics_str'] = review_df['topics'].apply(lambda x: ', '.join(x) if x else '')\n",
    "    \n",
    "    print(\"\\n=== RE-CLASSIFICATION RESULTS ===\")\n",
    "    for _, row in review_df.iterrows():\n",
    "        # Get original topics from results_df_clean\n",
    "        original_row = results_df_clean[results_df_clean['ticket_id'] == row['ticket_id']].iloc[0]\n",
    "        original_topics = set(evaluator.parse_topics(original_row['topics']))\n",
    "        new_topics = set(evaluator.parse_topics(row['topics']))\n",
    "        \n",
    "        print(f\"\\n{row['ticket_id']}:\")\n",
    "        print(f\"  Original: {original_topics}\")\n",
    "        print(f\"  New:      {new_topics}\")\n",
    "        print(f\"  Changed:  {'Yes' if original_topics != new_topics else 'No'}\")\n",
    "        \n",
    "    # Save updated results\n",
    "    review_df.to_csv('Review_Tickets_Reclassified.csv', index=False)\n",
    "    print(f\"\\nSaved re-classification results to Review_Tickets_Reclassified.csv\")\n",
    "else:\n",
    "    print(\"No tickets need review - all classifications are confident!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
